This documentation is a work in progress.

Applying the rule-based method to a new dataset proceeds in four
steps: processing the data into a suitable input format (usually the
most difficult and time-consuming piece;) filtering that data,
defining a population of candidate primitive rules, and setting up a
model object; sampling from the model; and analyzing the results.

1. Formatting the input data.

The method requires as input a suite of files matching those in the
sample_input directory. (For purposes of illustration, the filename in
the sample directory is given for each sample type; these filenames
are not mandatory.)

TBA: description of which dada2 output files these correspond to.

Required inputs:

- Abundance data (abundance_data.csv): Comma-delimited table of counts
  data. Each column corresponds to an OTU or RSV; each row to a
  sample. The first column specifies an identifier for each
  sample. The first row gives an identifier for each RSV/OTU.
  
- Sequences for each OTU/RSV (sequence_key.fa): FASTA file with one
  entry per RSV/OTU.  If the RSV/OTU identifiers in the abundance data
  file match sequences in this FASTA file, as is typically the case
  for DADA2 output, they will be renamed and referred to subsequently
  according to the corresponding IDs in this file.

- Table of subject and time point for each sample
  (sample_metadata.csv): Three-column CSV table with no header. First
  column corresponds to the sample identifiers in the abundance table,
  second column gives an identifier for the subject from which the
  sample was taken, third column gives a numeric value for the time at
  which the sample was taken.

- Table of data about each subject (subject_data.csv): CSV table with
  header row. The first column lists each subject (using the same
  identifiers as in the sample metadata file), and subsequent columns
  give values of categorical variables for each subject. The header
  specifies names for each field.  It might be important that the
  first field is 'subjectID'. The outcome of interest for each subject
  should be specified through this table.

Optional inputs:

- Taxonomic information for calibration and output purposes: TBA.

2. Preprocessing. 

Copy the sample file 'preprocess.py' and edit the parameters specified
in the section marked "# PARAMETERS", including the names of the input
files described above. Specify a prefix for the output files to be
generated by this run. Define the outcome variable, and which value of
the outcome should be considered 1 (vs 0). Specify the beginning and
end of the experiment and how many time intervals to chop the
experimental duration into. Note that overlapping windows can be
defined by specifying more intervals than desired, then setting
model_tmin to greater than the length of a single interval. (TODO:
expand on this.)

Currently, time windows in which one or more subjects have no
associated samples are excluded from consideration. To allow rules
with short time windows it is usually necessary to discard some
subjects which were not sampled densely enough. This can be done
automatically through the parameters
subject_min_observations_per_long_window and
intervals_per_long_window: subjects with less than
subject_min_observations_per_long_window associated samples in any
period of intervals_per_long_window consecutive time intervals will be
filtered out before defining the rule population. Tuning these
parameters involves a tradeoff between including as many subjects as
possible, which is critical for detecting associations, and sharpening
the temporal resolution.

These and the other filtering parameters generally need to be
determined empirically. In addition to filtering out data considered
objectively low-quality, it is necessary to keep the number of
included variables low enough that the resulting population of
primitive rules is computationally tractable. (TBA: discussion of
statistical issues.) Generally no more than 100K-200K primitive rules
are desired. The preprocessing script prints out the number of
variables, subjects and time points remaining after each filtering
step, to guide adjustments to the parameters.

3. Sampling.

Once an acceptable set of filtering parameters is arrived at, note the
name of the .pickle file saved at the end of the preprocessing step and run

python run_model.py input_pickle_file_with_model seconds_to_sample

Typically for populations of ~100K rules, using 24 processors on ERIS,
8 hours (8*3600) is a good initial choice.

4. Analysis.

Mostly TBA. For now, the run_model.py script prints a description of
the point summary RL and coefficients, and AUC and confusion matrix
values for the point summary and sampled ensemble.

5. Cross-validation.

TBA. 
